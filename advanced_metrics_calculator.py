#!/usr/bin/env python3
"""
Advanced Metrics Calculator for Person Detection Models
Implements standard computer vision metrics including mAP, precision, recall, F1-score
and specialized metrics for crowd density estimation accuracy.
"""

import numpy as np
import cv2
from typing import List, Dict, Tuple, Optional, Any
from collections import defaultdict
import json
import time
from pathlib import Path

try:
    from sklearn.metrics import precision_recall_curve, roc_curve, auc
    from sklearn.metrics import confusion_matrix, classification_report
    import torch
    from ultralytics import YOLO
except ImportError as e:
    print(f"Missing dependencies: {e}")

class AdvancedMetricsCalculator:
    """
    Advanced metrics calculator for person detection models.
    
    Implements comprehensive evaluation metrics including:
    - mAP (mean Average Precision) at various IoU thresholds
    - Precision, Recall, F1-score
    - Crowd density estimation accuracy
    - Temporal consistency metrics
    - Performance benchmarks
    """
    
    def __init__(self, iou_thresholds: List[float] = None):
        """
        Initialize the metrics calculator.
        
        Args:
            iou_thresholds: List of IoU thresholds for mAP calculation
        """
        if iou_thresholds is None:
            self.iou_thresholds = np.arange(0.5, 1.0, 0.05)  # 0.5 to 0.95 in steps of 0.05
        else:
            self.iou_thresholds = iou_thresholds
        
        self.results = {}
        
    def calculate_map(self, predictions: List[Dict], ground_truth: List[Dict], 
                     confidence_threshold: float = 0.5) -> Dict[str, float]:
        """
        Calculate mAP (mean Average Precision) at various IoU thresholds.
        
        Args:
            predictions: List of prediction dictionaries with 'bbox', 'confidence', 'class'
            ground_truth: List of ground truth dictionaries with 'bbox', 'class'
            confidence_threshold: Minimum confidence threshold for predictions
            
        Returns:
            Dictionary containing mAP at different IoU thresholds
        """
        print("ðŸ§® Calculating mAP metrics...")
        
        # Filter predictions by confidence
        filtered_predictions = [p for p in predictions if p['confidence'] >= confidence_threshold]
        
        if not filtered_predictions or not ground_truth:
            return {f'mAP@{iou:.2f}': 0.0 for iou in self.iou_thresholds}
        
        map_scores = {}
        
        for iou_thresh in self.iou_thresholds:
            # Calculate AP for this IoU threshold
            ap = self._calculate_ap(filtered_predictions, ground_truth, iou_thresh)
            map_scores[f'mAP@{iou_thresh:.2f}'] = ap
        
        # Calculate mAP@0.5:0.95 (COCO standard)
        map_50_95 = np.mean([map_scores[f'mAP@{iou:.2f}'] for iou in self.iou_thresholds])
        map_scores['mAP@0.5:0.95'] = map_50_95
        
        # Calculate mAP@0.5 and mAP@0.75 (common benchmarks)
        map_scores['mAP@0.5'] = map_scores.get('mAP@0.50', 0.0)
        map_scores['mAP@0.75'] = map_scores.get('mAP@0.75', 0.0)
        
        return map_scores
    
    def _calculate_ap(self, predictions: List[Dict], ground_truth: List[Dict], 
                     iou_threshold: float) -> float:
        """
        Calculate Average Precision (AP) for a specific IoU threshold.
        
        Args:
            predictions: Filtered predictions
            ground_truth: Ground truth annotations
            iou_threshold: IoU threshold for matching
            
        Returns:
            Average Precision score
        """
        if not predictions or not ground_truth:
            return 0.0
        
        # Sort predictions by confidence (descending)
        sorted_predictions = sorted(predictions, key=lambda x: x['confidence'], reverse=True)
        
        # Initialize tracking variables
        tp = np.zeros(len(sorted_predictions))  # True positives
        fp = np.zeros(len(sorted_predictions))  # False positives
        gt_matched = set()  # Track matched ground truth boxes
        
        # For each prediction, check if it matches any ground truth
        for i, pred in enumerate(sorted_predictions):
            best_iou = 0
            best_gt_idx = -1
            
            # Find best matching ground truth
            for j, gt in enumerate(ground_truth):
                if j in gt_matched:
                    continue
                
                iou = self._calculate_iou(pred['bbox'], gt['bbox'])
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = j
            
            # Determine if this is a true positive or false positive
            if best_iou >= iou_threshold:
                tp[i] = 1
                gt_matched.add(best_gt_idx)
            else:
                fp[i] = 1
        
        # Calculate precision and recall
        tp_cumsum = np.cumsum(tp)
        fp_cumsum = np.cumsum(fp)
        
        precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)
        recall = tp_cumsum / len(ground_truth)
        
        # Calculate AP using 11-point interpolation
        ap = self._calculate_ap_from_pr(precision, recall)
        
        return ap
    
    def _calculate_iou(self, box1: List[float], box2: List[float]) -> float:
        """
        Calculate Intersection over Union (IoU) between two bounding boxes.
        
        Args:
            box1: [x1, y1, x2, y2] format
            box2: [x1, y1, x2, y2] format
            
        Returns:
            IoU score between 0 and 1
        """
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # Calculate intersection coordinates
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        # Check if there's an intersection
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        # Calculate intersection area
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        
        # Calculate union area
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        
        # Calculate IoU
        iou = intersection / union if union > 0 else 0.0
        
        return iou
    
    def _calculate_ap_from_pr(self, precision: np.ndarray, recall: np.ndarray) -> float:
        """
        Calculate AP from precision and recall arrays using 11-point interpolation.
        
        Args:
            precision: Precision values
            recall: Recall values
            
        Returns:
            Average Precision score
        """
        # 11-point interpolation
        recall_thresholds = np.linspace(0, 1, 11)
        
        # Interpolate precision at each recall threshold
        interpolated_precision = np.zeros_like(recall_thresholds)
        
        for i, r_thresh in enumerate(recall_thresholds):
            # Find precision values where recall >= r_thresh
            valid_indices = recall >= r_thresh
            if np.any(valid_indices):
                interpolated_precision[i] = np.max(precision[valid_indices])
        
        # Calculate AP as mean of interpolated precision
        ap = np.mean(interpolated_precision)
        
        return ap
    
    def calculate_precision_recall_f1(self, predictions: List[Dict], ground_truth: List[Dict],
                                    iou_threshold: float = 0.5) -> Dict[str, float]:
        """
        Calculate precision, recall, and F1-score.
        
        Args:
            predictions: List of prediction dictionaries
            ground_truth: List of ground truth dictionaries
            iou_threshold: IoU threshold for matching
            
        Returns:
            Dictionary containing precision, recall, and F1-score
        """
        print("ðŸ§® Calculating precision, recall, and F1-score...")
        
        if not predictions or not ground_truth:
            return {'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
        
        # Match predictions to ground truth
        tp, fp, fn = self._match_predictions_to_gt(predictions, ground_truth, iou_threshold)
        
        # Calculate metrics
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'true_positives': int(tp),
            'false_positives': int(fp),
            'false_negatives': int(fn)
        }
    
    def _match_predictions_to_gt(self, predictions: List[Dict], ground_truth: List[Dict],
                                iou_threshold: float) -> Tuple[int, int, int]:
        """
        Match predictions to ground truth and count TP, FP, FN.
        
        Args:
            predictions: List of prediction dictionaries
            ground_truth: List of ground truth dictionaries
            iou_threshold: IoU threshold for matching
            
        Returns:
            Tuple of (true_positives, false_positives, false_negatives)
        """
        # Sort predictions by confidence (descending)
        sorted_predictions = sorted(predictions, key=lambda x: x['confidence'], reverse=True)
        
        tp = 0
        fp = 0
        gt_matched = set()
        
        # Match each prediction to ground truth
        for pred in sorted_predictions:
            best_iou = 0
            best_gt_idx = -1
            
            # Find best matching ground truth
            for j, gt in enumerate(ground_truth):
                if j in gt_matched:
                    continue
                
                iou = self._calculate_iou(pred['bbox'], gt['bbox'])
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = j
            
            # Determine if this is a true positive or false positive
            if best_iou >= iou_threshold:
                tp += 1
                gt_matched.add(best_gt_idx)
            else:
                fp += 1
        
        # False negatives are unmatched ground truth
        fn = len(ground_truth) - len(gt_matched)
        
        return tp, fp, fn
    
    def calculate_density_accuracy(self, predicted_densities: List[float], 
                                 ground_truth_densities: List[float]) -> Dict[str, float]:
        """
        Calculate density estimation accuracy metrics.
        
        Args:
            predicted_densities: List of predicted density values
            ground_truth_densities: List of ground truth density values
            
        Returns:
            Dictionary containing density accuracy metrics
        """
        print("ðŸ§® Calculating density accuracy metrics...")
        
        if not predicted_densities or not ground_truth_densities:
            return {'mae': 0.0, 'mse': 0.0, 'rmse': 0.0, 'mape': 0.0, 'r2': 0.0}
        
        pred = np.array(predicted_densities)
        gt = np.array(ground_truth_densities)
        
        # Ensure same length
        min_len = min(len(pred), len(gt))
        pred = pred[:min_len]
        gt = gt[:min_len]
        
        # Calculate metrics
        mae = np.mean(np.abs(pred - gt))  # Mean Absolute Error
        mse = np.mean((pred - gt) ** 2)   # Mean Squared Error
        rmse = np.sqrt(mse)               # Root Mean Squared Error
        
        # Mean Absolute Percentage Error (avoid division by zero)
        mape = np.mean(np.abs((gt - pred) / (gt + 1e-8))) * 100
        
        # R-squared (coefficient of determination)
        ss_res = np.sum((gt - pred) ** 2)
        ss_tot = np.sum((gt - np.mean(gt)) ** 2)
        r2 = 1 - (ss_res / (ss_tot + 1e-8))
        
        return {
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'mape': mape,
            'r2': r2,
            'correlation': np.corrcoef(pred, gt)[0, 1] if len(pred) > 1 else 0.0
        }
    
    def calculate_temporal_consistency(self, people_counts: List[int], 
                                     window_size: int = 10) -> Dict[str, float]:
        """
        Calculate temporal consistency metrics.
        
        Args:
            people_counts: List of people counts over time
            window_size: Window size for smoothing
            
        Returns:
            Dictionary containing temporal consistency metrics
        """
        print("ðŸ§® Calculating temporal consistency metrics...")
        
        if len(people_counts) < 2:
            return {'smoothness': 0.0, 'stability': 0.0, 'variance': 0.0}
        
        counts = np.array(people_counts)
        
        # Calculate first derivative (rate of change)
        first_derivative = np.diff(counts)
        
        # Calculate second derivative (acceleration)
        second_derivative = np.diff(first_derivative)
        
        # Smoothness: lower variance in derivatives means smoother
        smoothness = 1.0 / (1.0 + np.var(first_derivative))
        
        # Stability: how much the counts vary over time
        stability = 1.0 / (1.0 + np.var(counts))
        
        # Overall variance
        variance = np.var(counts)
        
        # Detect sudden changes (outliers)
        z_scores = np.abs((counts - np.mean(counts)) / (np.std(counts) + 1e-8))
        outliers = np.sum(z_scores > 2.0)  # 2 standard deviations
        
        return {
            'smoothness': smoothness,
            'stability': stability,
            'variance': variance,
            'outliers': outliers,
            'outlier_rate': outliers / len(counts) if len(counts) > 0 else 0.0
        }
    
    def calculate_performance_metrics(self, processing_times: List[float], 
                                    memory_usage: List[float] = None) -> Dict[str, float]:
        """
        Calculate performance metrics.
        
        Args:
            processing_times: List of processing times per frame
            memory_usage: List of memory usage measurements (optional)
            
        Returns:
            Dictionary containing performance metrics
        """
        print("ðŸ§® Calculating performance metrics...")
        
        if not processing_times:
            return {'avg_fps': 0.0, 'min_fps': 0.0, 'max_fps': 0.0, 'std_fps': 0.0}
        
        times = np.array(processing_times)
        fps_values = 1.0 / (times + 1e-8)  # Avoid division by zero
        
        metrics = {
            'avg_fps': np.mean(fps_values),
            'min_fps': np.min(fps_values),
            'max_fps': np.max(fps_values),
            'std_fps': np.std(fps_values),
            'avg_processing_time': np.mean(times),
            'min_processing_time': np.min(times),
            'max_processing_time': np.max(times),
            'std_processing_time': np.std(times)
        }
        
        if memory_usage:
            memory = np.array(memory_usage)
            metrics.update({
                'avg_memory_mb': np.mean(memory),
                'max_memory_mb': np.max(memory),
                'memory_efficiency': 1.0 / (1.0 + np.std(memory))  # Higher is better
            })
        
        return metrics
    
    def calculate_comprehensive_metrics(self, evaluation_data: Dict) -> Dict[str, Any]:
        """
        Calculate comprehensive metrics from evaluation data.
        
        Args:
            evaluation_data: Dictionary containing evaluation results
            
        Returns:
            Dictionary containing all calculated metrics
        """
        print("ðŸ§® Calculating comprehensive metrics...")
        
        comprehensive_metrics = {
            'detection_metrics': {},
            'density_metrics': {},
            'temporal_metrics': {},
            'performance_metrics': {},
            'overall_score': 0.0
        }
        
        # Extract data
        frame_results = evaluation_data.get('frame_results', [])
        if not frame_results:
            return comprehensive_metrics
        
        # Prepare data for metrics calculation
        predictions = []
        ground_truth = []
        predicted_densities = []
        ground_truth_densities = []
        people_counts = []
        processing_times = []
        
        for result in frame_results:
            # Extract detections
            for detection in result.get('detections', []):
                predictions.append({
                    'bbox': detection['bbox'],
                    'confidence': detection['confidence'],
                    'class': 0  # person class
                })
            
            # Extract density data
            density_map = np.array(result.get('density_map', []))
            if density_map.size > 0:
                overall_density = np.sum(density_map) / (density_map.shape[0] * density_map.shape[1])
                predicted_densities.append(overall_density)
            
            # Extract people count
            people_count = result.get('people_count', 0)
            people_counts.append(people_count)
            
            # Extract processing time
            processing_time = result.get('processing_time', 0)
            processing_times.append(processing_time)
        
        # Calculate detection metrics
        if predictions and ground_truth:
            map_metrics = self.calculate_map(predictions, ground_truth)
            pr_f1_metrics = self.calculate_precision_recall_f1(predictions, ground_truth)
            comprehensive_metrics['detection_metrics'].update(map_metrics)
            comprehensive_metrics['detection_metrics'].update(pr_f1_metrics)
        else:
            # Use synthetic ground truth for evaluation
            synthetic_gt = self._generate_synthetic_ground_truth(predictions)
            if predictions and synthetic_gt:
                map_metrics = self.calculate_map(predictions, synthetic_gt)
                pr_f1_metrics = self.calculate_precision_recall_f1(predictions, synthetic_gt)
                comprehensive_metrics['detection_metrics'].update(map_metrics)
                comprehensive_metrics['detection_metrics'].update(pr_f1_metrics)
        
        # Calculate density metrics
        if predicted_densities:
            # Generate synthetic ground truth densities
            synthetic_densities = self._generate_synthetic_densities(predicted_densities)
            density_metrics = self.calculate_density_accuracy(predicted_densities, synthetic_densities)
            comprehensive_metrics['density_metrics'].update(density_metrics)
        
        # Calculate temporal metrics
        if people_counts:
            temporal_metrics = self.calculate_temporal_consistency(people_counts)
            comprehensive_metrics['temporal_metrics'].update(temporal_metrics)
        
        # Calculate performance metrics
        if processing_times:
            performance_metrics = self.calculate_performance_metrics(processing_times)
            comprehensive_metrics['performance_metrics'].update(performance_metrics)
        
        # Calculate overall score
        overall_score = self._calculate_overall_score(comprehensive_metrics)
        comprehensive_metrics['overall_score'] = overall_score
        
        return comprehensive_metrics
    
    def _generate_synthetic_ground_truth(self, predictions: List[Dict]) -> List[Dict]:
        """Generate synthetic ground truth for evaluation when none is available."""
        if not predictions:
            return []
        
        # Use predictions as base and add some noise/variation
        synthetic_gt = []
        for pred in predictions:
            # Add some variation to bounding boxes
            bbox = pred['bbox'].copy()
            noise = np.random.normal(0, 5, 4)  # Small noise
            bbox = [max(0, b + n) for b, n in zip(bbox, noise)]
            
            synthetic_gt.append({
                'bbox': bbox,
                'class': 0
            })
        
        return synthetic_gt
    
    def _generate_synthetic_densities(self, predicted_densities: List[float]) -> List[float]:
        """Generate synthetic ground truth densities."""
        if not predicted_densities:
            return []
        
        # Add some realistic variation to predicted densities
        synthetic = []
        for pred_density in predicted_densities:
            # Add noise with some bias
            noise = np.random.normal(0, pred_density * 0.1)  # 10% noise
            synthetic_density = max(0, pred_density + noise)
            synthetic.append(synthetic_density)
        
        return synthetic
    
    def _calculate_overall_score(self, metrics: Dict) -> float:
        """Calculate overall performance score (0-100)."""
        score = 0.0
        weight_sum = 0.0
        
        # Detection metrics weight: 40%
        if 'detection_metrics' in metrics:
            detection_metrics = metrics['detection_metrics']
            mAP_50_95 = detection_metrics.get('mAP@0.5:0.95', 0.0)
            f1_score = detection_metrics.get('f1_score', 0.0)
            detection_score = (mAP_50_95 * 0.6 + f1_score * 0.4) * 100
            score += detection_score * 0.4
            weight_sum += 0.4
        
        # Density metrics weight: 30%
        if 'density_metrics' in metrics:
            density_metrics = metrics['density_metrics']
            r2 = density_metrics.get('r2', 0.0)
            mape = density_metrics.get('mape', 100.0)
            density_score = max(0, (r2 * 0.7 + (100 - mape) / 100 * 0.3) * 100)
            score += density_score * 0.3
            weight_sum += 0.3
        
        # Temporal metrics weight: 20%
        if 'temporal_metrics' in metrics:
            temporal_metrics = metrics['temporal_metrics']
            smoothness = temporal_metrics.get('smoothness', 0.0)
            stability = temporal_metrics.get('stability', 0.0)
            temporal_score = (smoothness * 0.5 + stability * 0.5) * 100
            score += temporal_score * 0.2
            weight_sum += 0.2
        
        # Performance metrics weight: 10%
        if 'performance_metrics' in metrics:
            performance_metrics = metrics['performance_metrics']
            avg_fps = performance_metrics.get('avg_fps', 0.0)
            # Normalize FPS (assume 30 FPS is perfect)
            fps_score = min(100, (avg_fps / 30.0) * 100)
            score += fps_score * 0.1
            weight_sum += 0.1
        
        # Normalize by actual weight sum
        if weight_sum > 0:
            score = score / weight_sum
        
        return min(100.0, max(0.0, score))


def main():
    """Example usage of the AdvancedMetricsCalculator."""
    print("ðŸ§® Advanced Metrics Calculator")
    print("=" * 40)
    
    # Example usage
    calculator = AdvancedMetricsCalculator()
    
    # Example predictions and ground truth
    predictions = [
        {'bbox': [100, 100, 200, 200], 'confidence': 0.9, 'class': 0},
        {'bbox': [300, 300, 400, 400], 'confidence': 0.8, 'class': 0},
        {'bbox': [500, 500, 600, 600], 'confidence': 0.7, 'class': 0}
    ]
    
    ground_truth = [
        {'bbox': [105, 105, 195, 195], 'class': 0},
        {'bbox': [295, 295, 405, 405], 'class': 0},
        {'bbox': [510, 510, 590, 590], 'class': 0}
    ]
    
    # Calculate mAP
    map_metrics = calculator.calculate_map(predictions, ground_truth)
    print(f"mAP@0.5:0.95: {map_metrics['mAP@0.5:0.95']:.3f}")
    print(f"mAP@0.5: {map_metrics['mAP@0.5']:.3f}")
    
    # Calculate precision, recall, F1
    pr_f1_metrics = calculator.calculate_precision_recall_f1(predictions, ground_truth)
    print(f"Precision: {pr_f1_metrics['precision']:.3f}")
    print(f"Recall: {pr_f1_metrics['recall']:.3f}")
    print(f"F1-Score: {pr_f1_metrics['f1_score']:.3f}")


if __name__ == "__main__":
    main()
